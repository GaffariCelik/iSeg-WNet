{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-HoCwEjpwgMy"},"outputs":[],"source":["!pip install tensorflow-addons==0.8.3\n","!pip install elasticdeform\n","!pip install time\n","!pip install SimpleITK\n","!pip install natsort"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"executionInfo":{"elapsed":966,"status":"ok","timestamp":1648792105836,"user":{"displayName":"Gaffari Ã‡elik","userId":"05436464457074694596"},"user_tz":-180},"id":"0cdDcI_zca9k","outputId":"7b3a5965-3f3e-4ea2-9c59-614daec6db34"},"outputs":[{"output_type":"stream","name":"stdout","text":["['../data/iseg-2017-2019_duzenlenmis/iSeg-2017+2019/train/1/t1.hdr', '../data/iseg-2017-2019_duzenlenmis/iSeg-2017+2019/train/10/t1.hdr', '../data/iseg-2017-2019_duzenlenmis/iSeg-2017+2019/train/11/t1.hdr', '../data/iseg-2017-2019_duzenlenmis/iSeg-2017+2019/train/12/t1.hdr', '../data/iseg-2017-2019_duzenlenmis/iSeg-2017+2019/train/13/t1.hdr', '../data/iseg-2017-2019_duzenlenmis/iSeg-2017+2019/train/14/t1.hdr', '../data/iseg-2017-2019_duzenlenmis/iSeg-2017+2019/train/15/t1.hdr', '../data/iseg-2017-2019_duzenlenmis/iSeg-2017+2019/train/16/t1.hdr', '../data/iseg-2017-2019_duzenlenmis/iSeg-2017+2019/train/2/t1.hdr', '../data/iseg-2017-2019_duzenlenmis/iSeg-2017+2019/train/3/t1.hdr', '../data/iseg-2017-2019_duzenlenmis/iSeg-2017+2019/train/4/t1.hdr', '../data/iseg-2017-2019_duzenlenmis/iSeg-2017+2019/train/5/t1.hdr', '../data/iseg-2017-2019_duzenlenmis/iSeg-2017+2019/train/6/t1.hdr', '../data/iseg-2017-2019_duzenlenmis/iSeg-2017+2019/train/7/t1.hdr', '../data/iseg-2017-2019_duzenlenmis/iSeg-2017+2019/train/8/t1.hdr', '../data/iseg-2017-2019_duzenlenmis/iSeg-2017+2019/train/9/t1.hdr']\n","Data Loaded...\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\na=0\\nfor Xbatch, Ybatch in train_gen:\\n    print(Xbatch.shape)\\n    print(Ybatch.shape)\\n    a=a+1\\nplt.imshow(Xbatch[1,:,:,30,0])    \\nprint(a)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}],"source":["import numpy as np\n","import tensorflow as tf\n","import nibabel as nib\n","import glob\n","import time\n","from tensorflow.keras.utils import to_categorical\n","\n","from sys import stdout\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpim\n","import elasticdeform as ed\n","from scipy.ndimage.interpolation import affine_transform\n","import concurrent.futures\n","\n","from tensorflow.keras.preprocessing.image import apply_affine_transform\n","\n","\n","from random import shuffle\n","\n","import os\n","\n","from natsort import natsorted, ns\n","\n","Nclasses = 4\n","classes = np.arange(Nclasses)\n","root_data='../data/iseg-2017-20199_duzenlenmis/iSeg-2017+2019/'\n","path_file='iseg_data_2017+2019_t1+t2_Dice_Loss_Zskore_Norm'\n","path_cv = \"results/cv5/\"+str(path_file)\n","if os.path.exists(path_cv)==False:\n","    os.mkdir(path_cv)\n","path_cv = \"test_Islemleri/data/Output/\"+str(path_file)\n","if os.path.exists(path_cv)==False:\n","    os.mkdir(path_cv)\n","path_cv = \"loss/\"+str(path_file)\n","if os.path.exists(path_cv)==False:\n","    os.mkdir(path_cv)\n","\n","\n","def data(data_selected):\n","    path = root_data+data_selected+\"/\"\n","    data_list = natsorted(os.listdir(path), alg=ns.PATH | ns.IGNORECASE)\n","    shuffle(data_list)\n","    path=path+'*/'\n","    # images lists\n","    t1_list = sorted(glob.glob(path+'*t1.hdr')) #'*t1.nii.gz'\n","    t2_list = sorted(glob.glob(path+'*t2.hdr'))\n","    seg_list = sorted(glob.glob(path+'*segm.hdr'))\n","    veri=[]\n","    for i in data_list:\n","        i=int(i)-1\n","        veri.append([t1_list[i], t2_list[i], seg_list[i]])\n","    return veri\n","\n","sets = {'train': [], 'valid': [], 'test': []}\n","sets['train']=data(data_selected='train')\n","print('Data Loaded...')\n","\n","def MinMax_Normalize(modality):\n","    X = modality\n","    brain = X\n","    brain_norm=X\n","    if ((np.max(brain)-np.min(brain))!=0):\n","      brain_norm = (brain - np.min(brain))/(np.max(brain)-np.min(brain))\n","    return brain_norm\n","\n","def Zskore_Normalize(modality):\n","    X = modality\n","    brain = X[X!=0]\n","    brain_norm = np.zeros_like(X) # background at -100\n","    if (np.std(brain)!=0):\n","      norm = (brain - np.mean(brain))/np.std(brain)\n","      brain_norm[X!=0] = norm\n","    else:\n","      brain_norm=X\n","    return brain_norm\n","\n","def load_img(img_files):\n","    ''' Load one image and its target form file\n","    '''\n","    N = len(img_files)\n","    # target\n","    y = nib.load(img_files[N-1]).get_fdata(dtype='float32')\n","    y=y.reshape(y.shape[0], y.shape[1],y.shape[2])\n","    y[y==10]=1\n","    y[y==150]=2\n","    y[y==250]=3\n","    X_norm = np.empty((y.shape[0], y.shape[1],y.shape[2], 2))\n","    y = y[6:134,30:174,80:208]#[0:256,0:112,0:256]\n","    for channel in range(N-1):\n","        X = nib.load(img_files[channel]).get_fdata(dtype='float32')\n","        X=X.reshape(X.shape[0], X.shape[1],X.shape[2])\n","        X_norm[:,:,:,channel] = Zskore_Normalize(X)#brain_norm\n","\n","    X_norm = X_norm[6:134,30:174,80:208,:]\n","    return X_norm, y\n","\n","class DataGenerator(tf.keras.utils.Sequence):\n","    'Generates data for Keras'\n","    def __init__(self, list_IDs, batch_size=1, dim=(128,144,128), n_channels=1, n_classes=4, shuffle=True,patch_size=64, n_patches=8):\n","        'Initialization'\n","        self.list_IDs = list_IDs\n","        self.batch_size = batch_size\n","        self.dim = dim\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","        self.shuffle = shuffle\n","        self.augmentation = augmentation\n","        self.patch_size = patch_size\n","        self.n_patches = n_patches\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        'Denotes the number of batches per epoch'\n","        return int(np.ceil(len(self.list_IDs) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        'Generate one batch of data'\n","        # Generate indexes of the batch\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","\n","        # Find list of IDs\n","        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n","\n","        # Generate data\n","        X, y = self.__data_generation(list_IDs_temp)\n","\n","        if index == self.__len__()-1:\n","            self.on_epoch_end()\n","\n","        return X, y\n","\n","    def on_epoch_end(self):\n","        self.indexes = np.arange(len(self.list_IDs))\n","        if self.shuffle == True:\n","            np.random.shuffle(self.indexes)\n","\n","    def __data_generation(self, list_IDs_temp):\n","        'Generates data containing batch_size samples'\n","        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n","        y = np.empty((self.batch_size, *self.dim))\n","        for i, IDs in enumerate(list_IDs_temp):\n","            X[i], y[i] = load_img(IDs)\n","        if self.augmentation == True:\n","            return X.astype('float32'), y\n","        else:\n","            return X.astype('float32'), to_categorical(y, self.n_classes)\n","\n","\n","train_gen = DataGenerator(sets['train'],batch_size=1, dim=(128,144,128), n_channels=2,n_classes=4,patch_size=1, n_patches=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OsaY5Ojxzgus"},"outputs":[],"source":["from tensorflow.keras.models import Model,load_model,model_from_json\n","from tensorflow.keras.layers import Input, Conv3D,UpSampling3D, Conv3DTranspose, Dropout,Activation, ReLU, LeakyReLU, Concatenate,BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras import backend as K\n","import os\n","from tensorflow_addons.layers import InstanceNormalization\n","import numpy as np\n","from utils2 import  get_dsc\n","import tensorflow.compat.v1 as tf\n","import config\n","\n","\n","from random import randint\n","root_1= ''\n","#root_data= ''\n","\n","class iSegWNet():\n","    def __init__(self, img_shape, seg_shape, Nclasses=4, Nfilter_start=8, depth=3,LAMBDA=5):\n","        self.img_shape = img_shape\n","        self.seg_shape = seg_shape\n","        self.Nfilter_start = Nfilter_start\n","        self.depth = depth\n","        self.Nclasses = Nclasses\n","        self.LAMBDA = LAMBDA\n","\n","        def diceLoss(y_true, y_pred):\n","            y_true = tf.convert_to_tensor(y_true, 'float32')\n","            y_pred = tf.convert_to_tensor(y_pred, y_true.dtype)\n","            num = tf.math.reduce_sum(tf.math.multiply(y_true, y_pred), axis=[0,1,2,3])\n","            den = tf.math.reduce_sum(tf.math.add(y_true, y_pred), axis=[0,1,2,3])+1e-5\n","            return 1-2*num/den\n","\n","\n","\n","        self.path = root_1+\"results/cv5/\"+str(path_file) # './Results_mri2seg_128_aug_lambda{}'.format(self.LAMBDA)\n","        if os.path.exists(self.path)==False:\n","            os.mkdir(self.path)\n","        if (os.path.exists(self.path+'/generator.h5')):\n","          import json\n","          with open(self.path+\"/generator.json\", \"r\") as f:\n","            model_json = json.load(f)\n","\n","          self.generator=model_from_json(model_json)\n","          self.generator.load_weights(self.path+\"/generator.h5\")\n","          self.generator.compile(loss=[diceLoss], optimizer=Adam(1e-4),metrics=['accuracy'])\n","        else:\n","          self.generator = self.WNet()\n","          self.generator.summary()\n","          self.generator.compile(loss=[diceLoss], optimizer=Adam(1e-4),metrics=['accuracy'])\n","    def WNet(self):\n","        inputs = Input(self.img_shape, name='input_image')\n","        def encoder_step(layer, Nf, inorm=True):\n","            Nf_1=Nf*2\n","            x = Conv3D(Nf, kernel_size=3, strides=2, kernel_initializer='he_normal', padding='same')(layer)\n","            #if inorm:\n","            x = InstanceNormalization()(x)\n","            x = LeakyReLU()(x)\n","            x=Dropout(0.2)(x)\n","            x = Conv3D(Nf_1, kernel_size=3,kernel_initializer='he_normal', padding='same')(x)\n","            x = InstanceNormalization()(x)\n","            x = LeakyReLU()(x)\n","            x=Dropout(0.2)(x)\n","            return x\n","        def bottlenek(layer, Nf):\n","            x = Conv3D(Nf, kernel_size=5, strides=2, kernel_initializer='he_normal', padding='same')(layer)\n","            x = InstanceNormalization()(x)\n","            x = LeakyReLU()(x)\n","            for i in range(4):\n","                y = Conv3D(Nf, kernel_size=5, strides=1, kernel_initializer='he_normal', padding='same')(x)\n","                x = InstanceNormalization()(y)\n","                x = LeakyReLU()(x)\n","                x = Concatenate()([x, y])\n","            return x\n","\n","        def decoder_step(layer, layer_to_concatenate, Nf):\n","            x = Conv3DTranspose(Nf, kernel_size=5, strides=2, padding='same', kernel_initializer='he_normal')(layer)\n","            x = InstanceNormalization()(x)\n","            x = LeakyReLU()(x)\n","            x = Concatenate()([x, layer_to_concatenate])\n","            x = Dropout(0.2)(x)\n","\n","            x = Conv3D(Nf, kernel_size=3,kernel_initializer='he_normal', padding='same')(x)\n","            x = InstanceNormalization()(x)\n","            x = LeakyReLU()(x)\n","            x=Dropout(0.2)(x)\n","            return x\n","\n","        x = inputs\n","        # encoder\n","        for wnet in range(2):\n","          layers_to_concatenate = []\n","          for d in range(self.depth-2):\n","              if d==0:\n","                  x = encoder_step(x, self.Nfilter_start*np.power(2,d), False)\n","              else:\n","                  x = encoder_step(x, self.Nfilter_start*np.power(2,d))\n","              layers_to_concatenate.append(x)\n","          # bottlenek\n","          x = bottlenek(x, self.Nfilter_start*np.power(2,self.depth-2))\n","          # decoder\n","          for d in  range(self.depth-3, -1, -1):\n","              x = decoder_step(x, layers_to_concatenate.pop(), self.Nfilter_start*np.power(2,d))\n","          # classifier\n","          if (wnet==0):\n","            x = Conv3DTranspose(filters=32, kernel_size=3, strides=2, padding='same', kernel_initializer='he_normal')(x) #filters=32\n","            x = InstanceNormalization()(x)\n","            x = LeakyReLU()(x)\n","          else:\n","            last = Conv3DTranspose(filters=self.Nclasses, kernel_size=3, strides=2, padding='same', activation='softmax', name='output2')(x)\n","          # Create model\n","        return Model(inputs=inputs, outputs=last, name='3D_Wnet')\n","\n","\n","    def train_step(self, Xbatch, Ybatch, mp=True, n_workers=16):\n","        # Generetor output\n","\n","        gen_loss = self.generator.fit(Xbatch, Ybatch,verbose=0)\n","        gen_output = self.generator.predict(Xbatch)\n","        dsc = get_dsc(labels=Ybatch,predictions=gen_output)\n","        return gen_loss,dsc\n","\n","    def valid_step(self, Xbatch, Ybatch, mp=True, n_workers=16):\n","        gen_loss = self.generator.evaluate(Xbatch, Ybatch, verbose=0)\n","        return gen_loss\n","    def save_model(self,path,durum='',itr=1):\n","        import json\n","        model_generator= self.generator.to_json()\n","        with open((path+\"/{}generator.json\").format(durum), \"w\") as json_file:\n","            json.dump(model_generator, json_file)\n","        self.generator.save_weights(path+\"/{}generator.h5\".format(durum))\n","        with open((path+\"/{}_generator.json\").format(itr), \"w\") as json_file:\n","            json.dump(model_generator, json_file)\n","        self.generator.save_weights(path+\"/{}_generator.h5\".format(itr))\n","        return 5\n","\n","    def run_toc(self,start_time):\n","      t_sec = round(time.time() - start_time)\n","      (t_min1, t_sec) = divmod(t_sec,60)\n","      (t_hour,t_min) = divmod(t_min1,60)\n","      return t_min1\n","\n","    def train(self,patch_size,train_time,nEpochs):\n","        print('Training process:')\n","        trends_train = tf.keras.callbacks.History()\n","        trends_train.epoch = []\n","\n","        trends_valid = tf.keras.callbacks.History()\n","        trends_valid.epoch = []\n","\n","       start_step=int(epoch_start)\n","        PERIOD_OF_TIME = time.time()\n","        start_time=time.time()\n","        cnt=0\n","        max_steps=2000\n","        for itr in range(start_step+1, max_steps+1):#FLAGS.max_steps):\n","          train_gen = DataGenerator(sets['train'], augmentation=False,batch_size=1,n_channels=2, patch_size=1, n_patches=1)\n","          for Xbatch, Ybatch in train_gen:\n","              gan_losses,s_dsc = self.train_step(Xbatch, Ybatch)\n","              gan_losses.history['loss'][0] *= self.LAMBDA\n","              train_status = '- step: {}/{} : loss: {:0.4f} - bgr:{:0.3f} csf:{:0.3f} gm:{:0.3f} wm:{:0.3f} '#+ \\\n","              txt=(train_status.format(itr, max_steps,gan_losses.history['loss'][0], s_dsc[0], s_dsc[1],s_dsc[2],s_dsc[3] #,s_dsc[4], #s_dsc[5], s_dsc[6], s_dsc[7], s_dsc[8], s_dsc[9], s_dsc[10],\n","                  ))\n","              print(txt)\n","\n","          with open(root_1+\"loss/\"+path_file+\"/train_loss.txt\", \"a+\") as f:\n","            f.write(txt+\"\\n\")\n","\n","          gecen_sure=self.run_toc(PERIOD_OF_TIME)\n","          if (gecen_sure>=60):\n","            PERIOD_OF_TIME = time.time()\n","            cnt+=60\n","            dd=self.save_model(path=self.path,durum='',itr=cnt)\n","            with open(root_1+\"loss/\"+path_file+\"/epoch.txt\", \"w\") as f:\n","              f.write(str(itr))\n","            print(\"Save checkpoint\")\n","            print(itr)\n","          if (cnt==train_time*60):\n","            break\n","\n","        np.save(self.path + '/history_train', trends_train.history)\n","        np.save(self.path + '/history_valid', trends_valid.history)\n","\"\"        return trends_train, trends_valid\n","\n","patch_size =[128,144,128] #list(map(int, FLAGS.patch_size.split(\",\")))\n","pz = patch_size[0]\n","py = patch_size[1]\n","px = patch_size[2]\n","\n","imShape = (pz, py, px, 2)\n","gtShape = (pz, py, px, 4)\n","iSegWNet = iSegWNet(imShape, gtShape,class_weights,Nfilter_start=64, depth=4)\n","\n","trends_train, trends_valid = iSegWNet.train(patch_size,3,300)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}